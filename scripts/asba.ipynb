{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0. Setup: Install Libraries & Mount Google Drive\n",
    "# Run this cell first!\n",
    "\n",
    "# Install necessary libraries if they are not already in the Colab environment\n",
    "!pip install pandas tqdm torch transformers scikit-learn spacy -q\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "\n",
    "# Mount Google Drive to access your files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\nLibraries installed/checked and Google Drive mounted (if authorized).\")\n",
    "print(\"IMPORTANT: Make sure to select 'Runtime' -> 'Change runtime type' -> 'GPU' for hardware accelerator.\")\n",
    "```python\n",
    "#@title 1. Import Libraries & Initial Configuration\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm # Progress bar for loops\n",
    "import torch # PyTorch for deep learning models\n",
    "import re # Regular expressions for text processing\n",
    "import gc # Garbage collector for memory management\n",
    "import spacy # For sentence splitting\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification # Hugging Face Transformers\n",
    "import os # For directory creation\n",
    "\n",
    "# Display options for pandas\n",
    "pd.set_option('display.max_colwidth', 200) # Show more text in DataFrame cells\n",
    "print(\"Libraries imported.\")\n",
    "```python\n",
    "#@title 2. Configuration Parameters & Path Setup\n",
    "\n",
    "# --- Main Configuration ---\n",
    "ASSISTANT_NAMES = [\"alexa\", \"google\"] # Datasets to process\n",
    "MODEL_DEBERTA_ABSA = 'yangheng/deberta-v3-base-absa-v1.1' # Chosen ABSA model\n",
    "MAX_SEQ_LENGTH = 512 # Max sequence length for the model\n",
    "PROCESS_ALL_REVIEWS = True # Set to True for full run, False to use NUM_REVIEWS_TO_PROCESS_DEBUG\n",
    "NUM_REVIEWS_TO_PROCESS_DEBUG = 100 # Number of reviews to process if PROCESS_ALL_REVIEWS is False (for quick debugging)\n",
    "\n",
    "# --- Google Drive Path Setup ---\n",
    "# IMPORTANT: Adjust this path to point to YOUR thesis folder on Google Drive\n",
    "# This folder should contain a 'results' subfolder with your input CSVs.\n",
    "# Example: If your thesis folder is \"MyThesisProject\" in the root of your Drive:\n",
    "# THESIS_ROOT_DRIVE = Path(\"/content/drive/MyDrive/MyThesisProject/\")\n",
    "\n",
    "THESIS_ROOT_DRIVE = Path(\"/content/drive/MyDrive/MSC/THESIS/thesis/\") # <--- !!! ADJUST THIS PATH !!!\n",
    "\n",
    "# --- Derived Paths ---\n",
    "input_dir = THESIS_ROOT_DRIVE / \"results\"\n",
    "output_dir = THESIS_ROOT_DRIVE / \"results\" / \"absa_full_results_colab\" # Separate output for Colab runs\n",
    "output_dir.mkdir(parents=True, exist_ok=True) # Create output directory if it doesn't exist\n",
    "\n",
    "print(f\"THESIS_ROOT on Drive set to: {THESIS_ROOT_DRIVE.resolve()}\")\n",
    "print(f\"Input directory: {input_dir.resolve()}\")\n",
    "print(f\"Output directory for ABSA results: {output_dir.resolve()}\")\n",
    "\n",
    "# --- Device Setup (GPU/CPU) ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    DEVICE_NAME_INFO = torch.cuda.get_device_name(0)\n",
    "    print(f\"Using GPU: {DEVICE_NAME_INFO}\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU. Processing will be slower. Ensure GPU is enabled in Colab Runtime settings.\")\n",
    "\n",
    "# --- Aspect Taxonomy (Copy from your previous script) ---\n",
    "taxonomy = {\n",
    "    \"Functionality & Performance\": [\n",
    "        \"command\", \"task\", \"function\", \"request\", \"execute\", \"perform\", \"play\", \"control\",\n",
    "        \"music\", \"timer\", \"alarm\", \"respond\", \"slow\", \"fast\", \"quick\", \"accurate\", \"ability\",\n",
    "        \"capability\", \"feature\", \"work\", \"operation\", \"answer\", \"weather\", \"news\", \"skill\",\n",
    "        \"search\", \"query\", \"song\", \"playlist\", \"speed\", \"performance\", \"reliable\", \"inconsistent\",\n",
    "        \"consistent\", \"accomplish\", \"smart\", \"intelligence\", \"stupid\", \"dumb\", \"basic\"\n",
    "    ],\n",
    "    \"Voice Recognition\": [\n",
    "        \"hear\", \"listen\", \"recognize\", \"understanding\", \"mic\", \"voice\", \"accent\", \"speech\",\n",
    "        \"microphone\", \"wake\", \"alexa\", \"hey google\", \"ok google\", \"command\", \"activation\",\n",
    "        \"trigger\", \"phrase\", \"call\", \"name\", \"hear me\", \"misheard\", \"mishear\", \"understand\",\n",
    "        \"detection\", \"sensitivity\", \"accent\", \"pronunciation\", \"dialect\", \"language\", \"recognition\"\n",
    "    ],\n",
    "    \"Knowledge Base\": [\n",
    "        \"answer\", \"knowledge\", \"info\", \"response\", \"fact\", \"question\", \"data\", \"correct\",\n",
    "        \"wrong\", \"information\", \"knowing\", \"research\", \"source\", \"accurate\", \"inaccurate\",\n",
    "        \"encyclopedia\", \"intelligence\", \"smart\", \"learn\", \"education\", \"informed\", \"wisdom\",\n",
    "        \"trivia\", \"facts\", \"content\", \"query\", \"request\", \"answer\", \"respond\"\n",
    "    ],\n",
    "    \"Integration & Ecosystem\": [\n",
    "        \"integrate\", \"connect\", \"compatible\", \"device\", \"home\", \"nest\", \"smart home\", \"ecosystem\",\n",
    "        \"philips\", \"hue\", \"lights\", \"thermostat\", \"tv\", \"television\", \"speaker\", \"app\", \"phone\",\n",
    "        \"smartphone\", \"skill\", \"third-party\", \"partner\", \"service\", \"platform\", \"sync\",\n",
    "        \"connection\", \"pair\", \"bluetooth\", \"wifi\", \"wireless\", \"smart\", \"bulb\", \"plug\", \"switch\",\n",
    "        \"camera\", \"doorbell\", \"lock\", \"appliance\", \"interoperability\", \"echo\", \"home mini\"\n",
    "    ],\n",
    "    \"Usability & Interface\": [\n",
    "        \"setup\", \"interface\", \"easy\", \"use\", \"design\", \"confusing\", \"intuitive\", \"simple\",\n",
    "        \"complicated\", \"difficult\", \"user-friendly\", \"accessibility\", \"accessible\", \"learn\",\n",
    "        \"instructions\", \"guide\", \"tutorial\", \"help\", \"clear\", \"straightforward\", \"configuration\",\n",
    "        \"settings\", \"customize\", \"personalize\", \"navigate\", \"interaction\", \"command structure\"\n",
    "    ],\n",
    "    \"Privacy & Security\": [\n",
    "        \"privacy\", \"data\", \"listening\", \"security\", \"surveillance\", \"record\", \"spy\", \"collect\",\n",
    "        \"tracking\", \"concern\", \"worry\", \"safe\", \"unsafe\", \"breach\", \"leak\", \"consent\", \"permission\",\n",
    "        \"trust\", \"trustworthy\", \"creepy\", \"scary\", \"suspicious\", \"watching\", \"monitoring\", \"gdpr\",\n",
    "        \"policy\", \"terms\", \"agreement\", \"encryption\", \"protected\", \"vulnerable\", \"hack\", \"risk\",\n",
    "        \"danger\", \"paranoid\", \"microphone\", \"camera\", \"recording\", \"personal\", \"information\", \"location\"\n",
    "    ],\n",
    "    \"Updates & Evolution\": [\n",
    "        \"update\", \"version\", \"bug\", \"feature\", \"release\", \"patch\", \"upgrade\", \"improve\",\n",
    "        \"improvement\", \"fix\", \"issue\", \"problem\", \"solved\", \"downgrade\", \"regression\", \"change\",\n",
    "        \"changed\", \"new\", \"added\", \"removed\", \"missing\", \"development\", \"roadmap\", \"progress\",\n",
    "        \"evolve\", \"evolution\", \"grow\", \"maturity\", \"mature\", \"immature\", \"beta\", \"alpha\", \"stable\"\n",
    "    ],\n",
    "    \"Support & Service\": [\n",
    "        \"support\", \"help\", \"service\", \"issue\", \"resolution\", \"customer\", \"contact\", \"call\",\n",
    "        \"phone\", \"email\", \"chat\", \"representative\", \"agent\", \"ticket\", \"case\", \"response\",\n",
    "        \"warranty\", \"replacement\", \"refund\", \"return\", \"satisfaction\", \"dissatisfaction\",\n",
    "        \"frustrated\", \"complaint\", \"feedback\", \"solve\", \"solution\", \"troubleshoot\", \"repair\"\n",
    "    ],\n",
    "    \"Social & Emotional Aspects\": [\n",
    "        \"personality\", \"character\", \"funny\", \"humor\", \"joke\", \"laugh\", \"fun\", \"entertaining\",\n",
    "        \"companion\", \"friend\", \"relationship\", \"emotion\", \"emotional\", \"human-like\", \"humanlike\",\n",
    "        \"personal\", \"personable\", \"warm\", \"cold\", \"robotic\", \"mechanical\", \"natural\", \"unnatural\",\n",
    "        \"conversation\", \"conversational\", \"chat\", \"talk\", \"dialogue\", \"interaction\", \"interactive\",\n",
    "        \"respond\", \"response\", \"reply\", \"engaging\", \"engage\", \"connection\", \"connect\", \"relate\"\n",
    "    ],\n",
    "    \"Personalization & Intelligence\": [\n",
    "        \"personalize\", \"customize\", \"preference\", \"learn\", \"adapt\", \"suggest\", \"recommendation\",\n",
    "        \"profile\", \"account\", \"user\", \"individual\", \"specific\", \"tailored\", \"custom\", \"habit\",\n",
    "        \"routine\", \"pattern\", \"predict\", \"predictive\", \"anticipate\", \"remember\", \"memory\",\n",
    "        \"context\", \"contextual\", \"awareness\", \"recognize\", \"familiar\", \"personal\", \"special\",\n",
    "        \"unique\", \"adjust\", \"adaptation\", \"history\", \"previous\", \"past\", \"experience\"\n",
    "    ]\n",
    "}\n",
    "print(\"\\nConfiguration and paths set.\")\n",
    "```python\n",
    "#@title 3. Load Models & Prepare Keyword Matching Tools\n",
    "\n",
    "# --- Load spaCy for sentence splitting ---\n",
    "print(\"Loading spaCy model (en_core_web_sm)...\")\n",
    "try:\n",
    "    nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "    # nlp_spacy.max_length = 1500000 # Uncomment and adjust if reviews are extremely long\n",
    "    print(\"spaCy model loaded.\")\n",
    "except OSError:\n",
    "    print(\"spaCy model 'en_core_web_sm' not found. Ensure it was installed in Cell 0.\")\n",
    "    raise\n",
    "\n",
    "# --- Prepare Keyword Matching Regex ---\n",
    "keyword_to_aspect_map = {}\n",
    "all_keywords_patterns = []\n",
    "for aspect_category, keywords in taxonomy.items():\n",
    "    for keyword in keywords:\n",
    "        kw_lower = keyword.lower() # Ensure keywords are lowercase for matching\n",
    "        keyword_to_aspect_map[kw_lower] = aspect_category\n",
    "        # Use word boundaries to match whole words only\n",
    "        all_keywords_patterns.append(r'\\b' + re.escape(kw_lower) + r'\\b')\n",
    "keyword_regex = re.compile('|'.join(all_keywords_patterns), re.IGNORECASE) # Case-insensitive matching\n",
    "print(\"Keyword regex compiled.\")\n",
    "\n",
    "# --- Load DeBERTa ABSA Model and Tokenizer ---\n",
    "# These will be loaded globally for use in the prediction function\n",
    "absa_tokenizer = None\n",
    "absa_model = None\n",
    "print(f\"Loading DeBERTa ABSA model: {MODEL_DEBERTA_ABSA}...\")\n",
    "try:\n",
    "    absa_tokenizer = AutoTokenizer.from_pretrained(MODEL_DEBERTA_ABSA)\n",
    "    absa_model = AutoModelForSequenceClassification.from_pretrained(MODEL_DEBERTA_ABSA).to(DEVICE)\n",
    "    absa_model.eval() # Set to evaluation mode (important for inference)\n",
    "    print(f\"DeBERTa ABSA model loaded successfully and moved to device: {DEVICE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DeBERTa ABSA model: {e}\")\n",
    "    # If model loading fails, we cannot proceed.\n",
    "    raise\n",
    "```python\n",
    "#@title 4. Helper Functions (Memory Release & Sentiment Prediction)\n",
    "\n",
    "def release_memory(model_to_del=None, tokenizer_to_del=None, custom_message=\"\"):\n",
    "    \"\"\"Releases memory occupied by specified components and clears CUDA cache.\"\"\"\n",
    "    if model_to_del:\n",
    "        del model_to_del\n",
    "    if tokenizer_to_del:\n",
    "        del tokenizer_to_del\n",
    "    gc.collect() # Force garbage collection\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache() # Clear PyTorch's CUDA memory cache\n",
    "    if custom_message:\n",
    "        print(custom_message)\n",
    "    # print(\"Memory release attempt complete.\")\n",
    "\n",
    "\n",
    "def predict_aspect_sentiment_batch(sentence_aspect_pairs, tokenizer, model, device_to_use, max_len, batch_size=32):\n",
    "    \"\"\"\n",
    "    Predicts sentiment for a batch of (sentence_text, aspect_category) pairs.\n",
    "    Returns a list of sentiment predictions.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    if not sentence_aspect_pairs:\n",
    "        return predictions\n",
    "\n",
    "    # Unzip the pairs\n",
    "    sentences, aspects = zip(*sentence_aspect_pairs)\n",
    "\n",
    "    try:\n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(\n",
    "            list(sentences),  # Convert tuple to list for tokenizer\n",
    "            list(aspects),    # Convert tuple to list for tokenizer\n",
    "            truncation=True,\n",
    "            padding='max_length', # Pad to the longest sequence in the batch or max_length\n",
    "            max_length=max_len,\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True\n",
    "        ).to(device_to_use)\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculations for inference\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        predicted_class_ids = torch.argmax(logits, dim=-1).cpu().tolist() # Move to CPU before converting to list\n",
    "\n",
    "        for class_id in predicted_class_ids:\n",
    "            pred_sentiment = model.config.id2label[class_id].capitalize()\n",
    "            if pred_sentiment not in ['Positive', 'Negative', 'Neutral']:\n",
    "                predictions.append('Neutral') # Fallback for unexpected labels\n",
    "            else:\n",
    "                predictions.append(pred_sentiment)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error during batched sentiment prediction: {e}. Returning 'Error' for batch.\")\n",
    "        # If batch fails, return \"Error\" for each item in the batch\n",
    "        return [\"Error\"] * len(sentence_aspect_pairs)\n",
    "\n",
    "print(\"Helper functions defined.\")\n",
    "```python\n",
    "#@title 5. Main Processing Loop (Full Scale ABSA)\n",
    "# This cell will process the data for each assistant.\n",
    "# It can take a long time depending on dataset size and hardware (CPU vs GPU).\n",
    "\n",
    "# Initialize tqdm for pandas (if you were to use .progress_apply(), not directly used here)\n",
    "# tqdm.pandas()\n",
    "\n",
    "# Define batch size for ABSA predictions\n",
    "# Adjust based on GPU memory. Larger batches are faster but use more VRAM.\n",
    "# For CPU, batching offers less speedup for this manual tokenization approach.\n",
    "PREDICTION_PROCESSING_BATCH_SIZE = 64 if DEVICE.type == 'cuda' else 8\n",
    "\n",
    "\n",
    "for assistant in ASSISTANT_NAMES:\n",
    "    print(f\"\\n=================================================================\")\n",
    "    print(f\"Processing full dataset for: {assistant.upper()}\")\n",
    "    print(f\"=================================================================\")\n",
    "    \n",
    "    input_file = input_dir / f\"{assistant}_with_topics.csv\"\n",
    "    output_file_path = output_dir / f\"{assistant}_full_absa_sentiments_colab.csv\"\n",
    "\n",
    "    if not input_file.exists():\n",
    "        print(f\"Input file not found for {assistant}: {input_file}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_reviews_full = pd.read_csv(input_file)\n",
    "        print(f\"Loaded {len(df_reviews_full)} reviews for {assistant}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {input_file}: {e}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Validate required columns\n",
    "    required_cols = ['reviewId', 'clean_content', 'at']\n",
    "    if not all(col in df_reviews_full.columns for col in required_cols):\n",
    "        print(f\"One or more required columns ({', '.join(required_cols)}) missing in {input_file}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    df_reviews_full = df_reviews_full.dropna(subset=required_cols)\n",
    "    df_reviews_full['clean_content'] = df_reviews_full['clean_content'].astype(str)\n",
    "\n",
    "    # --- Subsetting Logic for Debugging or Full Run ---\n",
    "    if PROCESS_ALL_REVIEWS:\n",
    "        df_to_process = df_reviews_full.copy()\n",
    "        print(f\"Processing all {len(df_to_process)} reviews for {assistant}.\")\n",
    "    else:\n",
    "        if len(df_reviews_full) > NUM_REVIEWS_TO_PROCESS_DEBUG:\n",
    "            df_to_process = df_reviews_full.head(NUM_REVIEWS_TO_PROCESS_DEBUG).copy()\n",
    "            print(f\"DEBUG MODE: Processing a subset of {len(df_to_process)} reviews for {assistant}.\")\n",
    "        else:\n",
    "            df_to_process = df_reviews_full.copy()\n",
    "            print(f\"DEBUG MODE: Dataset for {assistant} has {len(df_to_process)} reviews (less than debug limit). Processing all available.\")\n",
    "    # --- End of Subsetting Logic ---\n",
    "\n",
    "    all_aspect_sentiments_data = [] # To store dictionaries for final DataFrame\n",
    "    sentence_aspect_pairs_batch = [] # To collect pairs for batch prediction\n",
    "    metadata_for_batch = [] # To store corresponding (reviewId, sentence, aspect, keyword, timestamp)\n",
    "\n",
    "    print(f\"Identifying aspects and preparing for sentiment prediction for {assistant}...\")\n",
    "    # Outer loop for reviews\n",
    "    for index, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=f\"Reviews ({assistant})\"):\n",
    "        review_id_val = row['reviewId']\n",
    "        review_text = row['clean_content']\n",
    "        review_timestamp = row['at']\n",
    "\n",
    "        if not review_text.strip():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            review_text_for_spacy = review_text[:nlp_spacy.max_length] if len(review_text) > nlp_spacy.max_length else review_text\n",
    "            doc_spacy = nlp_spacy(review_text_for_spacy)\n",
    "\n",
    "            # Inner loop for sentences in a review\n",
    "            for sent in doc_spacy.sents:\n",
    "                sentence_text = sent.text.strip()\n",
    "                if len(sentence_text) < 10: # Skip very short sentences\n",
    "                    continue\n",
    "\n",
    "                found_keywords_in_sentence = keyword_regex.findall(sentence_text)\n",
    "                aspects_processed_for_this_sentence = set()\n",
    "\n",
    "                # Innermost loop for keywords in a sentence\n",
    "                for keyword in found_keywords_in_sentence:\n",
    "                    keyword_lower = keyword.lower()\n",
    "                    if keyword_lower in keyword_to_aspect_map:\n",
    "                        aspect_category = keyword_to_aspect_map[keyword_lower]\n",
    "                        \n",
    "                        if aspect_category not in aspects_processed_for_this_sentence:\n",
    "                            # Add to batch for prediction\n",
    "                            sentence_aspect_pairs_batch.append((sentence_text, aspect_category))\n",
    "                            metadata_for_batch.append({\n",
    "                                'reviewId': review_id_val,\n",
    "                                'sentence_text': sentence_text,\n",
    "                                'identified_aspect': aspect_category,\n",
    "                                'matched_keyword': keyword_lower,\n",
    "                                'timestamp': review_timestamp\n",
    "                            })\n",
    "                            aspects_processed_for_this_sentence.add(aspect_category)\n",
    "\n",
    "                            # If batch is full, predict and clear\n",
    "                            if len(sentence_aspect_pairs_batch) >= PREDICTION_PROCESSING_BATCH_SIZE:\n",
    "                                predicted_sentiments_batch = predict_aspect_sentiment_batch(\n",
    "                                    sentence_aspect_pairs_batch, \n",
    "                                    absa_tokenizer, \n",
    "                                    absa_model,\n",
    "                                    DEVICE, \n",
    "                                    MAX_SEQ_LENGTH,\n",
    "                                    batch_size=PREDICTION_PROCESSING_BATCH_SIZE # Pass configured batch size\n",
    "                                )\n",
    "                                # Combine metadata with predictions\n",
    "                                for i, meta_item in enumerate(metadata_for_batch):\n",
    "                                    meta_item['aspect_sentiment'] = predicted_sentiments_batch[i]\n",
    "                                    all_aspect_sentiments_data.append(meta_item)\n",
    "                                \n",
    "                                sentence_aspect_pairs_batch = [] # Clear batch\n",
    "                                metadata_for_batch = []      # Clear metadata\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Major error processing review ID {review_id_val}. Details: {e}. Skipping this review.\")\n",
    "            # Clear current batch if an error occurs at review level to avoid misalignment\n",
    "            sentence_aspect_pairs_batch = []\n",
    "            metadata_for_batch = []\n",
    "            continue\n",
    "            \n",
    "    # Process any remaining items in the last batch\n",
    "    if sentence_aspect_pairs_batch:\n",
    "        predicted_sentiments_batch = predict_aspect_sentiment_batch(\n",
    "            sentence_aspect_pairs_batch, \n",
    "            absa_tokenizer, \n",
    "            absa_model,\n",
    "            DEVICE, \n",
    "            MAX_SEQ_LENGTH,\n",
    "            batch_size=PREDICTION_PROCESSING_BATCH_SIZE\n",
    "        )\n",
    "        for i, meta_item in enumerate(metadata_for_batch):\n",
    "            meta_item['aspect_sentiment'] = predicted_sentiments_batch[i]\n",
    "            all_aspect_sentiments_data.append(meta_item)\n",
    "\n",
    "    # --- Saving Results ---\n",
    "    if all_aspect_sentiments_data:\n",
    "        df_results = pd.DataFrame(all_aspect_sentiments_data)\n",
    "        print(f\"\\nSaving {len(df_results)} aspect-sentiment pairs for {assistant} to {output_file_path}...\")\n",
    "        try:\n",
    "            df_results.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "            print(f\"Saved successfully for {assistant}.\")\n",
    "            print(\"\\nSample of results:\")\n",
    "            print(df_results.head())\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results for {assistant} to CSV: {e}\")\n",
    "    else:\n",
    "        print(f\"No aspect-sentiment pairs found or generated for {assistant}.\")\n",
    "\n",
    "    # Optional: Release memory for the DataFrame if it's very large and you need RAM for the next assistant\n",
    "    if 'df_results' in locals():\n",
    "        del df_results\n",
    "    if 'df_to_process' in locals():\n",
    "        del df_to_process\n",
    "    if 'df_reviews_full' in locals():\n",
    "        del df_reviews_full\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"\\n--- Full-scale ABSA processing complete for all specified assistants. ---\")\n",
    "```python\n",
    "#@title 6. Final Cleanup (Optional)\n",
    "# Run this cell if you want to explicitly release the main models from memory\n",
    "# (Colab usually handles this when the runtime disconnects, but good practice if re-running parts)\n",
    "\n",
    "print(\"\\nReleasing main ABSA model and tokenizer from memory (if they exist)...\")\n",
    "if 'absa_model' in globals() and absa_model is not None:\n",
    "    release_memory(model_to_del=absa_model, custom_message=\"ABSA model released.\")\n",
    "    del absa_model # Ensure variable is removed\n",
    "else:\n",
    "    print(\"absa_model not found or already released.\")\n",
    "\n",
    "if 'absa_tokenizer' in globals() and absa_tokenizer is not None:\n",
    "    release_memory(tokenizer_to_del=absa_tokenizer, custom_message=\"ABSA tokenizer released.\")\n",
    "    del absa_tokenizer # Ensure variable is removed\n",
    "else:\n",
    "    print(\"absa_tokenizer not found or already released.\")\n",
    "\n",
    "if 'nlp_spacy' in globals() and nlp_spacy is not None:\n",
    "    del nlp_spacy # spaCy model doesn't need explicit GPU release but good to clear\n",
    "    gc.collect()\n",
    "    print(\"spaCy model variable deleted.\")\n",
    "else:\n",
    "    print(\"nlp_spacy not found or already released.\")\n",
    "\n",
    "print(\"Final cleanup attempt complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
